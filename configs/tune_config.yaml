# =============================================================================
# Ray Tune 기본 설정
# =============================================================================
name: "llm_hyperparameter_tuning"
num_samples: 50  # 총 실험 횟수
max_concurrent_trials: 1  # 동시 실행할 트라이얼 수

# 리소스 설정
resources_per_trial:
  cpu: 4
  gpu: 1

# =============================================================================
# 스케줄러 설정
# =============================================================================
scheduler_type: "ASHA"  # "ASHA", "PBT", "BOHB", "MedianStopping"

# ASHA 스케줄러 설정
asha_config:
  metric: "eval_loss"
  mode: "min"
  max_t: 2000  # 2000스텝으로 설정
  grace_period: 200  # max_t의 10% 정도
  reduction_factor: 3
  brackets: 1

# =============================================================================
# 서치 알고리즘 설정
# =============================================================================
search_algorithm: "bayesian"  # "random", "grid", "bayesian", "hyperopt"

# =============================================================================
# 하이퍼파라미터 서치 스페이스 정의
# =============================================================================

# 학습률 관련
learning_rate:
  type: "loguniform"
  low: 1e-6
  high: 1e-3

warmup_ratio:
  type: "uniform"
  low: 0.01
  high: 0.1

weight_decay:
  type: "uniform"
  low: 0.001
  high: 0.1

lr_scheduler_type:
  type: "choice"
  values: ["cosine", "cosine_with_restarts", "polynomial"]  # "linear"

# 배치 크기 관련
per_device_train_batch_size:
  type: "choice"
  values: [1, 2]

gradient_accumulation_steps:
  type: "choice"
  values: [1, 2, 4, 8, 16, 32]  # GLOBAL_BATCH_SIZE // (per_device_train_batch_size * NUM_DEVICES)

# LoRA 관련 파라미터
lora_r:
  type: "choice"
  values: [8, 16, 32, 64, 128]

lora_alpha:
  type: "choice"
  values: [8, 16, 32, 64, 128, 256]

lora_dropout:
  type: "uniform"
  low: 0.0
  high: 0.3

# 기존 fixed_params에서 target_modules 제거하고
lora_target_modules:
  type: "choice"
  values:
    - ["q_proj", "v_proj"]  # 기본
    - ["q_proj", "k_proj", "v_proj", "o_proj"]  # attention만
    - ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]  # 전체
    - ["gate_proj", "up_proj", "down_proj"]  # MLP만

# =============================================================================
# 고정 파라미터 (튜닝하지 않는 파라미터들)
# =============================================================================
fixed_params:
  seed: 2025
  num_train_epochs: 5
  eval_strategy: "steps"
  save_strategy: "steps"
  logging_steps: 25
  save_total_limit: 1
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  push_to_hub: false
  optim: "adamw_torch"
  bias: "none"
  init_lora_weights: true
  label_names: ["labels"]

# =============================================================================
# 출력 및 로깅 설정
# =============================================================================
output_dir: "./outputs/tune_results"
local_dir: "./ray_results"

# 체크포인트 설정
checkpoint_config:
  checkpoint_score_attr: "eval_loss"
  checkpoint_freq: 1
  num_to_keep: 2

# =============================================================================
# 조기 중단 설정
# =============================================================================
early_stopping:
  enabled: true
  patience: 3
  min_delta: 0.001

# =============================================================================
# 리포팅 설정
# =============================================================================
progress_reporter:
  metric_columns: ["eval_loss", "eval_accuracy", "training_iteration"]
  max_progress_rows: 20
  max_error_rows: 5
  max_column_length: 20

# =============================================================================
# 베스트 모델 선택 기준
# =============================================================================
best_model_selection:
  metric: "eval_loss"
  mode: "min"
  scope: "all"  # "all", "last", "avg"
