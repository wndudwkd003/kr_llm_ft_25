#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Unsloth + TRL ê¸°ë°˜ DPO í•™ìŠµ í´ë˜ìŠ¤ ì˜ˆì‹œ

ì‚¬ìš©ë²• ê°œìš”
-----------
cm = ConfigManager(...)  # ê¸°ì¡´ê³¼ ë™ì¼í•œ ì„¤ì • ê°ì²´
trainer = UnslothDPOTrainer(cm)
trainer.setup_model()                # ëª¨ë¸/í† í¬ë‚˜ì´ì €/LoRA ì„¸íŒ…
train_ds, eval_ds = trainer.prepare_dataset()  # DPOìš© ë°ì´í„°ì…‹ ë¡œë“œ
stats = trainer.train(train_ds, eval_ds)       # í•™ìŠµ ìˆ˜í–‰
trainer.save()                      # ì–´ëŒ‘í„°/í† í¬ë‚˜ì´ì € ì €ì¥

í•„ìš” ConfigManager í‚¤ ì˜ˆì‹œ
--------------------------
cm.model.model_id          : str  (HF ëª¨ë¸ ID)
cm.model.max_seq_length    : int
cm.model.dtype             : str  ("bfloat16" | "float16" | "float32" ë“±)

cm.lora.r                  : int
cm.lora.target_modules     : list[str]
cm.lora.lora_alpha         : int
cm.lora.lora_dropout       : float
cm.lora.bias               : str   ("none", "lora_only", ...)

cm.system.seed             : int
cm.system.data_raw_path    : str   (ë°ì´í„° ê²½ë¡œ)
cm.system.output_dir       : str   (ëª¨ë¸ ì €ì¥ ê²½ë¡œ)

cm.dpo                     : dict  (transformers.TrainingArguments ì— ë“¤ì–´ê°ˆ ì¸ìë“¤)
  â”œâ”€ "output_dir": str
  â”œâ”€ "per_device_train_batch_size": int
  â”œâ”€ "per_device_eval_batch_size": int
  â”œâ”€ "learning_rate": float
  â”œâ”€ ...

cm.dpo_hparams             : dict (DPOTrainer ì „ìš© í•˜ì´í¼íŒŒë¼ë¯¸í„°)
  â”œâ”€ "beta": float            # DPO ì˜¨ë„
  â”œâ”€ "loss_type": str        # "sigmoid" | "ipo" | "hinge" ë“± (TRL>=0.9)
  â”œâ”€ "label_smoothing": float
  â”œâ”€ "reference_free": bool   # True ì‹œ ref model ì—†ì´ í•™ìŠµ
  â””â”€ ê¸°íƒ€ DPO ê´€ë ¨ ì¸ì

ë°ì´í„° í¬ë§· (train.json / dev.json)
-----------------------------------
ê° ë¼ì¸ì€ ì•„ë˜ í‚¤ë¥¼ í¬í•¨í•œ JSON ê°ì²´:
{
  "prompt":   "ì§ˆë¬¸ ë˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
  "chosen":   "ì„ í˜¸ë˜ëŠ”(ì •ë‹µ) ì‘ë‹µ",
  "rejected": "ëœ ì„ í˜¸ë˜ëŠ”(ì˜¤ë‹µ) ì‘ë‹µ"
}

ì£¼ìš” ì˜ì¡´ ë¼ì´ë¸ŒëŸ¬ë¦¬
--------------------
unsloth>=0.6, transformers>=4.41, trl>=0.9
"""
import unsloth
#from __future__ import annotations
import os
import json
from typing import Tuple, Optional, List, Dict

from unsloth import FastLanguageModel
from trl import DPOTrainer
from transformers import TrainingArguments, PreTrainedTokenizerBase
from torch.utils.data import Dataset

from src.data.dpo_dataset import DPODataset
from src.configs.config_manager import ConfigManager
from src.data.dpo_dataset import DataCollatorForDPODataset
from dataclasses import asdict

from peft import PeftModel   
from datasets import Dataset as HFDataset

class UnslothDPOTrainer:
    """Unsloth + TRL DPO í•™ìŠµ ë˜í¼ í´ë˜ìŠ¤."""

    def __init__(self, config_manager: ConfigManager):
        self.cm: ConfigManager = config_manager
        self.model = None
        self.tokenizer: Optional[PreTrainedTokenizerBase] = None
        self.trainer: Optional[DPOTrainer] = None
        self.ref_model = None  # í•„ìš” ì‹œ ìˆ˜ë™ ì„¤ì • ê°€ëŠ¥

    # ------------------------------------------------------------------
    # 1) ëª¨ë¸/í† í¬ë‚˜ì´ì € ì¤€ë¹„ & LoRA ì ìš©
    # ------------------------------------------------------------------
    def setup_model(self):
        """FastLanguageModel ë¡œë“œ ë° LoRA ì„¤ì •."""
        self.model, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=self.cm.model.model_id,
            max_seq_length=self.cm.model.max_seq_length,
            dtype=self.cm.model.dtype,
            load_in_4bit=False,
            load_in_8bit=False,
            full_finetuning=False,
        )

        # pad_token ì—†ìœ¼ë©´ eosë¡œ ì„¸íŒ…
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "right"

        # LoRA ì ìš©
        lora_adapter_dir = os.path.join(self.cm.dpo.dpo_model_path, "lora_adapter")

        self.model = PeftModel.from_pretrained(
            self.model,
            model_id=lora_adapter_dir,
            is_trainable=True,  # DPO ë‹¨ê³„ì—ì„œ LoRA íŒŒë¼ë¯¸í„°ë¥¼ ë‹¤ì‹œ í•™ìŠµ
        )

        print(f"[INFO] Loaded pretrained LoRA from {lora_adapter_dir}")

    # ------------------------------------------------------------------
    # 2) ë°ì´í„°ì…‹ ì¤€ë¹„
    # ------------------------------------------------------------------
    def prepare_dataset(self) -> Tuple[HFDataset, HFDataset]:
        train_path = os.path.join(self.cm.system.data_raw_path, "train.json")
        dev_path   = os.path.join(self.cm.system.data_raw_path, "dev.json")

        # â‘  ê¸°ì¡´ Torchâ€‘style DPODataset ìƒì„±
        torch_train = DPODataset(train_path, self.tokenizer, max_length=self.cm.model.max_seq_length)
        torch_dev   = DPODataset(dev_path,  self.tokenizer, max_length=self.cm.model.max_seq_length)

        # â‘¡ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ğŸ¤—Â Dataset ìœ¼ë¡œ ë˜í•‘
        train_hf = HFDataset.from_list(list(torch_train))
        dev_hf   = HFDataset.from_list(list(torch_dev))

        return train_hf, dev_hf

    # ------------------------------------------------------------------
    # 3) í•™ìŠµ ë£¨í‹´
    # ------------------------------------------------------------------
    def train(self, train_dataset: DPODataset, eval_dataset: DPODataset):
        """DPOTrainerë¥¼ ì´ìš©í•œ í•™ìŠµ ìˆ˜í–‰."""
        dpo_cfg = asdict(self.cm.dpo)

        dpo_only_keys = [
            "dpo_model_path", "beta", "loss_type", "label_smoothing",
            "reference_free", "precompute_ref_log_probs",
            "max_length", "max_prompt_length", "max_target_length",
            "padding_value", "model_init_kwargs", "ref_model_init_kwargs",
            "generate_during_eval", "model_adapter_name", "ref_adapter_name",
            "reference_free", "disable_dropout", "use_liger_loss",
            "label_pad_token_id", "max_completion_length", "truncation_mode",
            "use_logits_to_keep", "padding_free", "loss_type",
            "beta", "use_weighting", "f_divergence_type",
            "f_alpha_divergence_coef", "dataset_num_proc", "tools",
            "sync_ref_model", "precompute_ref_batch_size"
            
        ]
        dpo_kwargs = {k: dpo_cfg.pop(k) for k in dpo_only_keys if k in dpo_cfg}

        # â”€â”€ Unslothê°€ ìš”êµ¬í•˜ëŠ” í•„ë“œ ê¸°ë³¸ê°’ í…Œì´ë¸” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        unsloth_extra_defaults = dict(
            padding_value            = self.tokenizer.pad_token_id,
            model_init_kwargs        = {"dtype": self.cm.model.dtype},
            ref_model_init_kwargs    = None,
            generate_during_eval     = False,
            model_adapter_name        = None,
            ref_adapter_name          = None,
            reference_free           = False,  # DPOTrainerê°€ ì§€ì›í•˜ëŠ” ê²½ìš°
            disable_dropout          = False,  # DPOTrainerê°€ ì§€ì›í•˜ëŠ” ê²½ìš°
            use_liger_loss           = False,  # DPOTrainerê°€ ì§€ì›í•˜ëŠ” ê²½ìš°
            label_pad_token_id       = self.tokenizer.pad_token_id,
            max_prompt_length        = self.cm.dpo.max_prompt_length,
            max_completion_length    = self.cm.dpo.max_target_length,
            max_length               = self.cm.model.max_seq_length,
            truncation_mode          = "longest_first",  # DPOTrainerê°€ ì§€ì›í•˜ëŠ” ê²½ìš°
            precompute_ref_log_probs = self.cm.dpo.precompute_ref_log_probs,
            use_logits_to_keep       = False,  # DPOTrainerê°€ ì§€ì›í•˜ëŠ” ê²½ìš°
            padding_free             = False,  # DPOTrainerê°€ ì§€ì›í•˜ëŠ” ê²½ìš°
            loss_type                = "sigmoid",  # DPOTrainerê°€ ì§€ì›í•˜ëŠ” ê²½ìš°'
            beta                     = 0.1,  # DPOTrainerê°€ ì§€ì›í•˜ëŠ” ê²½ìš°
            label_smoothing          = 0.0,  # DPOTrainerê°€ ì§€ì›í•˜ëŠ” ê²½ìš°
            use_weighting            = False,  # DPOTrainerê°€ ì§€ì›í•˜ëŠ” ê²½ìš°
            f_divergence_type        = "kl",  # DPOTrainerê°€ ì§€ì›í•˜ëŠ” ê²½ìš°
            f_alpha_divergence_coef  = 0.0,  # DPOTrainerê°€ ì§€ì›í•˜ëŠ” ê²½ìš°
            dataset_num_proc         = 1,  # DPOTrainerê°€ ì§€ì›í•˜ëŠ” ê²½ìš°
            tools                    = None,  # DPOTrainerê°€ ì§€ì›í•˜ëŠ” ê²½ìš°
            sync_ref_model           = False,  # DPOTrainerê°€ ì§€ì›í•˜ëŠ” ê²½ìš°
            precompute_ref_batch_size = 16,  # DPOTrainerê°€ ì§€ì›í•˜ëŠ” ê²½ìš°
        )

        # â”€â”€ TrainingArguments ìƒì„± í›„ ëˆ„ë½ëœ í•„ë“œ ëª¨ë‘ ì£¼ì… â”€â”€â”€â”€
        training_args = TrainingArguments(**dpo_cfg)
        for k, v in unsloth_extra_defaults.items():
            if not hasattr(training_args, k):
                setattr(training_args, k, dpo_kwargs.get(k, v))

        # TRLì—ì„œ ì œê³µí•˜ëŠ” ì „ìš© Collator (label í† í° ë¶„ë¦¬ ë“±)
        data_collator = DataCollatorForDPODataset(
            tokenizer=self.tokenizer,
            max_length=self.cm.model.max_seq_length,
            pad_to_multiple_of=8,
        )

        self.trainer = DPOTrainer(
            model=self.model,
            ref_model=self.ref_model,  # Noneì¼ ê²½ìš° ë‚´ë¶€ì—ì„œ ë³µì œí•˜ì—¬ reference modelë¡œ ì‚¬ìš©
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            # dpo parameters
            beta=dpo_kwargs.get("beta"),
            loss_type=dpo_kwargs.get("loss_type"),
            label_smoothing=dpo_kwargs.get("label_smoothing"),
            reference_free=dpo_kwargs.get("reference_free"),
        )

        train_result = self.trainer.train()
        return train_result

    # ------------------------------------------------------------------
    # 4) ì €ì¥
    # ------------------------------------------------------------------
    def save(self, output_dir: Optional[str] = None, merge_adapter: bool = False):
        """ëª¨ë¸/ì–´ëŒ‘í„°/í† í¬ë‚˜ì´ì € ì €ì¥.

        Args:
            output_dir (str): ì €ì¥ ê²½ë¡œ. Noneì´ë©´ cm.dpo["output_dir"] ì‚¬ìš©
            merge_adapter (bool): Trueë©´ LoRA weightë¥¼ base modelì— ë³‘í•© í›„ ì €ì¥
        """
        save_dir = output_dir or self.cm.dpo.get("output_dir", self.cm.system.output_dir)
        os.makedirs(save_dir, exist_ok=True)

        if merge_adapter:
            # UnslothëŠ” FastLanguageModel.save_pretrained ì„ ì‚¬ìš©í•´ ë³‘í•© ê°€ëŠ¥
            FastLanguageModel.save_pretrained(
                self.model,
                save_dir,
                tokenizer=self.tokenizer,
                merge_lora=True,
            )
        else:
            # ì–´ëŒ‘í„°ë§Œ ì €ì¥
            self.trainer.model.save_pretrained(save_dir)
            self.tokenizer.save_pretrained(save_dir)

    # ------------------------------------------------------------------
    # 5) (ì„ íƒ) ref_modelì„ ì™¸ë¶€ì—ì„œ ì œê³µí•˜ê³  ì‹¶ì„ ë•Œ
    # ------------------------------------------------------------------
    def load_reference_model(self, model_path: Optional[str] = None):
        """Reference modelì„ ëª…ì‹œì ìœ¼ë¡œ ë¡œë“œí•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©.

        Args:
            model_path (str): HF ëª¨ë¸ ID ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ. Noneì´ë©´ self.cm.model.model_id ì¬ì‚¬ìš©.
        """
        if model_path is None:
            model_path = self.cm.model.model_id

        ref_model, _ = FastLanguageModel.from_pretrained(
            model_name=model_path,
            max_seq_length=self.cm.model.max_seq_length,
            dtype=self.cm.model.dtype,
            load_in_4bit=False,
            load_in_8bit=False,
            full_finetuning=False,
        )
        # LoRA ë¯¸ì ìš© / Gradient ë¹„í™œì„±í™”ë¡œ ê³ ì •
        ref_model.eval()
        for p in ref_model.parameters():
            p.requires_grad_(False)
        self.ref_model = ref_model